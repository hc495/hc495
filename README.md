Working on NN/LLM Interpretability in Japan. Refer to https://www.hakaze-c.com/.

## List of Official Experiment Codes of Hakaze Cho's Works

**Mechanism of Task-oriented Information Removal in In-context Learning** [Paper](https://arxiv.org/abs/2509.21012) [Repository](https://github.com/hc495/Verb_subspace) [ICLR 2026]

**Binary Autoencoder for Mechanistic Interpretability of Large Language Models** [Paper](https://arxiv.org/abs/2509.20997) [Repository](https://github.com/hc495/Binary_Autoencoder)

**Revisiting In-context Learning Inference Circuit in Large Language Models** [Paper](https://arxiv.org/abs/2410.04468) [Repository](https://github.com/hc495/ICL_Circuit) [ICLR 2025 Poster] 

**Token-based Decision Criteria Are Suboptimal in In-context Learning** [Paper](https://arxiv.org/abs/2406.16535) [Repository](https://github.com/hc495/Hidden_Calibration) [NAACL 2025 Main] 

**Mechanistic Fine-tuning for In-context Learning** [Paper](https://arxiv.org/abs/2505.14233) [Repository](https://github.com/hc495/ICL_head_tuning) [EMNLP 2025 BlackBox NLP workshop] 

**StaICC: Standardized Toolkit for In-context Classification** [Paper](https://arxiv.org/abs/2501.15708) [Repository](https://github.com/hc495/StaICC) [PyPl](https://pypi.org/project/StaICC/)

**NoisyICL: A Little Noise in Model Parameters Calibrates In-context Learning** [Paper](https://arxiv.org/abs/2402.05515) [Repository](https://github.com/hc495/NoisyICL)
