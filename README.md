Working on NN/LLM Interpretability in Japan. Refer to https://www.hakaze-c.com/.

## List of Official Experiment Codes of Hakaze Cho's Works

[ICLR 2026] [Mechanism of Task-oriented Information Removal in In-context Learning](https://arxiv.org/abs/2509.21012) [Repo](https://github.com/hc495/Verb_subspace)

[Binary Autoencoder for Mechanistic Interpretability of Large Language Models](https://arxiv.org/abs/2509.20997) [Repo](https://github.com/hc495/Binary_Autoencoder)

[ICLR 2025 Poster] [Revisiting In-context Learning Inference Circuit in Large Language Models](https://arxiv.org/abs/2410.04468) [Repo](https://github.com/hc495/ICL_Circuit)

[NAACL 2025 Main] [Token-based Decision Criteria Are Suboptimal in In-context Learning](https://arxiv.org/abs/2406.16535) [Repo](https://github.com/hc495/Hidden_Calibration)

[EMNLP 2025 BlackBox NLP workshop] [Mechanistic Fine-tuning for In-context Learning](https://arxiv.org/abs/2505.14233) [Repo](https://github.com/hc495/ICL_head_tuning)

[StaICC: Standardized Toolkit for In-context Classification](https://arxiv.org/abs/2501.15708) [Repo](https://github.com/hc495/StaICC) [PyPl](https://pypi.org/project/StaICC/)

[NoisyICL: A Little Noise in Model Parameters Calibrates In-context Learning](https://arxiv.org/abs/2402.05515) [Repo](https://github.com/hc495/NoisyICL)
