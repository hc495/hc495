Working on NN/LLM Interpretability in Japan. Refer to https://www.hakaze-c.com/.

## List of Official Experiment Codes of Hakaze Cho's Works

[ICLR 2026] **Mechanism of Task-oriented Information Removal in In-context Learning** [Paper](https://arxiv.org/abs/2509.21012) [Repository](https://github.com/hc495/Verb_subspace)

**Binary Autoencoder for Mechanistic Interpretability of Large Language Models** [Paper](https://arxiv.org/abs/2509.20997) [Repository](https://github.com/hc495/Binary_Autoencoder)

[ICLR 2025 Poster] **Revisiting In-context Learning Inference Circuit in Large Language Models** [Paper](https://arxiv.org/abs/2410.04468) [Repository](https://github.com/hc495/ICL_Circuit)

[NAACL 2025 Main] **Token-based Decision Criteria Are Suboptimal in In-context Learning** [Paper](https://arxiv.org/abs/2406.16535) [Repository](https://github.com/hc495/Hidden_Calibration)

[EMNLP 2025 BlackBox NLP workshop] **Mechanistic Fine-tuning for In-context Learning** [Paper](https://arxiv.org/abs/2505.14233) [Repository](https://github.com/hc495/ICL_head_tuning)

**StaICC: Standardized Toolkit for In-context Classification** [Paper](https://arxiv.org/abs/2501.15708) [Repository](https://github.com/hc495/StaICC) [PyPl](https://pypi.org/project/StaICC/)

**NoisyICL: A Little Noise in Model Parameters Calibrates In-context Learning** [Paper](https://arxiv.org/abs/2402.05515) [Repository](https://github.com/hc495/NoisyICL)
